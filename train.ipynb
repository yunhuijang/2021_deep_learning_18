{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f20ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2718988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fusion4268/anaconda3/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262435ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561c979e9b2341db83b7818e9a80bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f3d5b6cb5047b790936f6d1c21a276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2d21c783df400a96cc4559099979ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4f17897a2d43d8924becadce027ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "data = pd.read_csv('~/2021_deep_learning_18/dataset/train_final_tokenized.csv')\n",
    "pr=data.profile_report()\n",
    "pr.to_file('pr_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac950045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['Token'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5fe30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Token</th>\n",
       "      <th>BERTToken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-LRB- The film -RRB- tackles the topic of rela...</td>\n",
       "      <td>['film', 'tackles', 'topic', 'relationships', ...</td>\n",
       "      <td>[101, 1011, 1048, 15185, 1011, 1996, 2143, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Lavishly , exhilaratingly tasteless .</td>\n",
       "      <td>['lavishly', 'exhilaratingly', 'tasteless']</td>\n",
       "      <td>[101, 22689, 2135, 1010, 4654, 26415, 15172, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>It is also beautifully acted .</td>\n",
       "      <td>['also', 'beautifully', 'acted']</td>\n",
       "      <td>[101, 2009, 2003, 2036, 17950, 6051, 1012, 102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>But , like Silence , it 's a movie that gets u...</td>\n",
       "      <td>['like', 'silence', \"'s\", 'movie', 'gets', 'sk...</td>\n",
       "      <td>[101, 2021, 1010, 2066, 4223, 1010, 2009, 1005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>It 's been made with an innocent yet fervid co...</td>\n",
       "      <td>[\"'s\", 'made', 'innocent', 'yet', 'fervid', 'c...</td>\n",
       "      <td>[101, 2009, 1005, 1055, 2042, 2081, 2007, 2019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>11539</td>\n",
       "      <td>3</td>\n",
       "      <td>Although Frailty fits into a classic genre , i...</td>\n",
       "      <td>['although', 'frailty', 'fits', 'classic', 'ge...</td>\n",
       "      <td>[101, 2348, 25737, 3723, 16142, 2046, 1037, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>11540</td>\n",
       "      <td>1</td>\n",
       "      <td>Mediocre fable from Burkina Faso .</td>\n",
       "      <td>['mediocre', 'fable', 'burkina', 'faso']</td>\n",
       "      <td>[101, 19960, 3695, 16748, 28458, 2013, 23089, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>11541</td>\n",
       "      <td>4</td>\n",
       "      <td>Like all great films about a life you never kn...</td>\n",
       "      <td>['like', 'great', 'films', 'life', 'never', 'k...</td>\n",
       "      <td>[101, 2066, 2035, 2307, 3152, 2055, 1037, 2166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>11542</td>\n",
       "      <td>4</td>\n",
       "      <td>Those who are n't put off by the film 's auste...</td>\n",
       "      <td>[\"n't\", 'put', 'film', \"'s\", 'austerity', 'fin...</td>\n",
       "      <td>[101, 2216, 2040, 2024, 1050, 1005, 1056, 2404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11543</th>\n",
       "      <td>11543</td>\n",
       "      <td>4</td>\n",
       "      <td>An ambitious movie that , like Shiner 's organ...</td>\n",
       "      <td>['ambitious', 'movie', 'like', 'shiner', \"'s\",...</td>\n",
       "      <td>[101, 2019, 12479, 3185, 2008, 1010, 2066, 123...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11544 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Category                                           Sentence  \\\n",
       "0          0         3  -LRB- The film -RRB- tackles the topic of rela...   \n",
       "1          1         2              Lavishly , exhilaratingly tasteless .   \n",
       "2          2         4                     It is also beautifully acted .   \n",
       "3          3         1  But , like Silence , it 's a movie that gets u...   \n",
       "4          4         2  It 's been made with an innocent yet fervid co...   \n",
       "...      ...       ...                                                ...   \n",
       "11539  11539         3  Although Frailty fits into a classic genre , i...   \n",
       "11540  11540         1                 Mediocre fable from Burkina Faso .   \n",
       "11541  11541         4  Like all great films about a life you never kn...   \n",
       "11542  11542         4  Those who are n't put off by the film 's auste...   \n",
       "11543  11543         4  An ambitious movie that , like Shiner 's organ...   \n",
       "\n",
       "                                                   Token  \\\n",
       "0      ['film', 'tackles', 'topic', 'relationships', ...   \n",
       "1            ['lavishly', 'exhilaratingly', 'tasteless']   \n",
       "2                       ['also', 'beautifully', 'acted']   \n",
       "3      ['like', 'silence', \"'s\", 'movie', 'gets', 'sk...   \n",
       "4      [\"'s\", 'made', 'innocent', 'yet', 'fervid', 'c...   \n",
       "...                                                  ...   \n",
       "11539  ['although', 'frailty', 'fits', 'classic', 'ge...   \n",
       "11540           ['mediocre', 'fable', 'burkina', 'faso']   \n",
       "11541  ['like', 'great', 'films', 'life', 'never', 'k...   \n",
       "11542  [\"n't\", 'put', 'film', \"'s\", 'austerity', 'fin...   \n",
       "11543  ['ambitious', 'movie', 'like', 'shiner', \"'s\",...   \n",
       "\n",
       "                                               BERTToken  \n",
       "0      [101, 1011, 1048, 15185, 1011, 1996, 2143, 101...  \n",
       "1      [101, 22689, 2135, 1010, 4654, 26415, 15172, 2...  \n",
       "2        [101, 2009, 2003, 2036, 17950, 6051, 1012, 102]  \n",
       "3      [101, 2021, 1010, 2066, 4223, 1010, 2009, 1005...  \n",
       "4      [101, 2009, 1005, 1055, 2042, 2081, 2007, 2019...  \n",
       "...                                                  ...  \n",
       "11539  [101, 2348, 25737, 3723, 16142, 2046, 1037, 44...  \n",
       "11540  [101, 19960, 3695, 16748, 28458, 2013, 23089, ...  \n",
       "11541  [101, 2066, 2035, 2307, 3152, 2055, 1037, 2166...  \n",
       "11542  [101, 2216, 2040, 2024, 1050, 1005, 1056, 2404...  \n",
       "11543  [101, 2019, 12479, 3185, 2008, 1010, 2066, 123...  \n",
       "\n",
       "[11544 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2814dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_dis, input_mask, segment_ids, labels, num_labels):\n",
    "    \n",
    "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segement_ids=segement_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict = True)\n",
    "    \n",
    "    output_layer = bert_outputs[\"poole_outputs\"]\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\"output_weights\", [num_labes, hidden_size],\n",
    "                                    initializer =tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    output_bias = tf.get_variable(\"output_bias\", [num_labels],\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        \n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=1)\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=float32)\n",
    "        \n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        \n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "        \n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2900ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder (num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        \n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segement_ids = features[\"segement_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        \n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        \n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "            s_predicting, input_dis, input_mask, segment_ids, labels, num_labels)\n",
    "            \n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "            loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu = False)   \n",
    "            \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics,f1_score(label_ids, predicted_labels)\n",
    "                return {\"accuracy\": accuracy, \"f1_score\": f1_score}\n",
    "            \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            \n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_meric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            \n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "        \n",
    "    return model_fn           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1748b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "class Base:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def read_data(self, fname: str, lower_case: bool=False, \n",
    "                  colnames=['Category', 'Sentence', 'Token', 'BERTToken']) -> pd.DataFrame:\n",
    "        \n",
    "        df = pd.read_csv(fname, sep='\\t', header = None, names=colnames)\n",
    "        return df\n",
    "    \n",
    "    def accuracy(self, df: pd.DataFrame) -> None:\n",
    "        acc = accuracy_score(df['Category'], df['pred'])*100\n",
    "        f1 = f1_score(df['Category'], df['pred'], average='macro')\n",
    "        print(\"Accuracy: {}\\nMacro F1-score: {}\".format(acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456676c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSentiment(Base):\n",
    "    def __init__(self, model_file: str=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def score(self, text: str) -> int:\n",
    "        #sentiment scoring technique\n",
    "        return 1\n",
    "        \n",
    "    def predict(self, train_file: None, test_file: str, lower_case: bool) -> pd.DataFrame:\n",
    "        #DataFrame that applies the sentiment scoring method on each row of the test set\n",
    "        df = self.read_data(test_file, lower_case)\n",
    "        df['pred'] = df['text'].apply(self.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e17ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT-Small\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f37651",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(data) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2802acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/output'\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecdb4279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1080361c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "label_list = [1,2,3,4,5]\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55fadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68b5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281cd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d886ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05201023",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a445f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88717470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1375     <bert.run_classifier.InputExample object at 0x...\n",
       "13246    <bert.run_classifier.InputExample object at 0x...\n",
       "12326    <bert.run_classifier.InputExample object at 0x...\n",
       "18341    <bert.run_classifier.InputExample object at 0x...\n",
       "20838    <bert.run_classifier.InputExample object at 0x...\n",
       "                               ...                        \n",
       "20327    <bert.run_classifier.InputExample object at 0x...\n",
       "24276    <bert.run_classifier.InputExample object at 0x...\n",
       "15499    <bert.run_classifier.InputExample object at 0x...\n",
       "5699     <bert.run_classifier.InputExample object at 0x...\n",
       "12088    <bert.run_classifier.InputExample object at 0x...\n",
       "Length: 5000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d685eb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1dde2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee8eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fusion4268/anaconda3/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fusion4268/anaconda3/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] not an altogether bad start for the program - - but what a slap in the face to real law enforcement . the worst part of the series is that it attempts to bill itself as reality fare - - and is anything but . men and women that de ##dicate their lives to the enforcement of laws deserve better than this . what is next , medical school in a minute ? char ##o performing lip ##o ? charles gr ##od ##in assisting on a hip replacement ? c ' mon . . . show a little respect . even the citizens of mu ##nc ##ie are outing the program as staged . police academy = high school gym ? poor editing ( how many [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] not an altogether bad start for the program - - but what a slap in the face to real law enforcement . the worst part of the series is that it attempts to bill itself as reality fare - - and is anything but . men and women that de ##dicate their lives to the enforcement of laws deserve better than this . what is next , medical school in a minute ? char ##o performing lip ##o ? charles gr ##od ##in assisting on a hip replacement ? c ' mon . . . show a little respect . even the citizens of mu ##nc ##ie are outing the program as staged . police academy = high school gym ? poor editing ( how many [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2025 2019 10462 2919 2707 2005 1996 2565 1011 1011 2021 2054 1037 14308 1999 1996 2227 2000 2613 2375 7285 1012 1996 5409 2112 1997 1996 2186 2003 2008 2009 4740 2000 3021 2993 2004 4507 13258 1011 1011 1998 2003 2505 2021 1012 2273 1998 2308 2008 2139 16467 2037 3268 2000 1996 7285 1997 4277 10107 2488 2084 2023 1012 2054 2003 2279 1010 2966 2082 1999 1037 3371 1029 25869 2080 4488 5423 2080 1029 2798 24665 7716 2378 13951 2006 1037 5099 6110 1029 1039 1005 12256 1012 1012 1012 2265 1037 2210 4847 1012 2130 1996 4480 1997 14163 12273 2666 2024 26256 1996 2565 2004 9813 1012 2610 2914 1027 2152 2082 9726 1029 3532 9260 1006 2129 2116 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2025 2019 10462 2919 2707 2005 1996 2565 1011 1011 2021 2054 1037 14308 1999 1996 2227 2000 2613 2375 7285 1012 1996 5409 2112 1997 1996 2186 2003 2008 2009 4740 2000 3021 2993 2004 4507 13258 1011 1011 1998 2003 2505 2021 1012 2273 1998 2308 2008 2139 16467 2037 3268 2000 1996 7285 1997 4277 10107 2488 2084 2023 1012 2054 2003 2279 1010 2966 2082 1999 1037 3371 1029 25869 2080 4488 5423 2080 1029 2798 24665 7716 2378 13951 2006 1037 5099 6110 1029 1039 1005 12256 1012 1012 1012 2265 1037 2210 4847 1012 2130 1996 4480 1997 14163 12273 2666 2024 26256 1996 2565 2004 9813 1012 2610 2914 1027 2152 2082 9726 1029 3532 9260 1006 2129 2116 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] without \" mental ana ##ch ##ron ##ism \" , this film which i would like to find in dvd offer an extraordinary diving in the vital and mental context of thought of the people before the \" di ##sen ##chan ##tment of the world \" . that , there is thirty years , a director and a scenario writer could test one such empathy and such a romantic truth to do it of them masterpiece leaves me as ##tou ##nding . it would be necessary to be able to see and re - examine it film for better seizing than the temporal and cultural distance us to make lose of capacity to be included / understood , analyze and finally to accept of such or such [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] without \" mental ana ##ch ##ron ##ism \" , this film which i would like to find in dvd offer an extraordinary diving in the vital and mental context of thought of the people before the \" di ##sen ##chan ##tment of the world \" . that , there is thirty years , a director and a scenario writer could test one such empathy and such a romantic truth to do it of them masterpiece leaves me as ##tou ##nding . it would be necessary to be able to see and re - examine it film for better seizing than the temporal and cultural distance us to make lose of capacity to be included / understood , analyze and finally to accept of such or such [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2302 1000 5177 9617 2818 4948 2964 1000 1010 2023 2143 2029 1045 2052 2066 2000 2424 1999 4966 3749 2019 9313 9404 1999 1996 8995 1998 5177 6123 1997 2245 1997 1996 2111 2077 1996 1000 4487 5054 14856 21181 1997 1996 2088 1000 1012 2008 1010 2045 2003 4228 2086 1010 1037 2472 1998 1037 11967 3213 2071 3231 2028 2107 26452 1998 2107 1037 6298 3606 2000 2079 2009 1997 2068 17743 3727 2033 2004 24826 15683 1012 2009 2052 2022 4072 2000 2022 2583 2000 2156 1998 2128 1011 11628 2009 2143 2005 2488 24681 2084 1996 15850 1998 3451 3292 2149 2000 2191 4558 1997 3977 2000 2022 2443 1013 5319 1010 17908 1998 2633 2000 5138 1997 2107 2030 2107 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2302 1000 5177 9617 2818 4948 2964 1000 1010 2023 2143 2029 1045 2052 2066 2000 2424 1999 4966 3749 2019 9313 9404 1999 1996 8995 1998 5177 6123 1997 2245 1997 1996 2111 2077 1996 1000 4487 5054 14856 21181 1997 1996 2088 1000 1012 2008 1010 2045 2003 4228 2086 1010 1037 2472 1998 1037 11967 3213 2071 3231 2028 2107 26452 1998 2107 1037 6298 3606 2000 2079 2009 1997 2068 17743 3727 2033 2004 24826 15683 1012 2009 2052 2022 4072 2000 2022 2583 2000 2156 1998 2128 1011 11628 2009 2143 2005 2488 24681 2084 1996 15850 1998 3451 3292 2149 2000 2191 4558 1997 3977 2000 2022 2443 1013 5319 1010 17908 1998 2633 2000 5138 1997 2107 2030 2107 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ten minutes worth of story stretched out into the better part of two hours . when nothing of any significance had happened at the halfway point i should have left . but , ever hopeful , i stayed . and left with a feeling of guilt for having wasted the time . acting was ok , but the story line is so transparent and weak . the script is about as lame as it could get , but again , stretching out the ten minute plot doesn ' t leave a whole lot of room for good dialogue . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ten minutes worth of story stretched out into the better part of two hours . when nothing of any significance had happened at the halfway point i should have left . but , ever hopeful , i stayed . and left with a feeling of guilt for having wasted the time . acting was ok , but the story line is so transparent and weak . the script is about as lame as it could get , but again , stretching out the ten minute plot doesn ' t leave a whole lot of room for good dialogue . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2702 2781 4276 1997 2466 7121 2041 2046 1996 2488 2112 1997 2048 2847 1012 2043 2498 1997 2151 7784 2018 3047 2012 1996 8576 2391 1045 2323 2031 2187 1012 2021 1010 2412 17772 1010 1045 4370 1012 1998 2187 2007 1037 3110 1997 8056 2005 2383 13842 1996 2051 1012 3772 2001 7929 1010 2021 1996 2466 2240 2003 2061 13338 1998 5410 1012 1996 5896 2003 2055 2004 20342 2004 2009 2071 2131 1010 2021 2153 1010 10917 2041 1996 2702 3371 5436 2987 1005 1056 2681 1037 2878 2843 1997 2282 2005 2204 7982 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2702 2781 4276 1997 2466 7121 2041 2046 1996 2488 2112 1997 2048 2847 1012 2043 2498 1997 2151 7784 2018 3047 2012 1996 8576 2391 1045 2323 2031 2187 1012 2021 1010 2412 17772 1010 1045 4370 1012 1998 2187 2007 1037 3110 1997 8056 2005 2383 13842 1996 2051 1012 3772 2001 7929 1010 2021 1996 2466 2240 2003 2061 13338 1998 5410 1012 1996 5896 2003 2055 2004 20342 2004 2009 2071 2131 1010 2021 2153 1010 10917 2041 1996 2702 3371 5436 2987 1005 1056 2681 1037 2878 2843 1997 2282 2005 2204 7982 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i thought the movie was actually pretty good . i enjoyed the acting and it moved along well . the director seemed to really grasp the story he was trying to tell . i have to see the big budget one coming out today , obviously they had a lot more money to throw at it but was very watch ##able . when you see a movie like this for a small budget you have to take that in to account when you are viewing it . there were some things that could of been better but most are budget related . the acting was pretty good the f / x and stunts were well done . a couple of stand ##outs were the guy who [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i thought the movie was actually pretty good . i enjoyed the acting and it moved along well . the director seemed to really grasp the story he was trying to tell . i have to see the big budget one coming out today , obviously they had a lot more money to throw at it but was very watch ##able . when you see a movie like this for a small budget you have to take that in to account when you are viewing it . there were some things that could of been better but most are budget related . the acting was pretty good the f / x and stunts were well done . a couple of stand ##outs were the guy who [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2245 1996 3185 2001 2941 3492 2204 1012 1045 5632 1996 3772 1998 2009 2333 2247 2092 1012 1996 2472 2790 2000 2428 10616 1996 2466 2002 2001 2667 2000 2425 1012 1045 2031 2000 2156 1996 2502 5166 2028 2746 2041 2651 1010 5525 2027 2018 1037 2843 2062 2769 2000 5466 2012 2009 2021 2001 2200 3422 3085 1012 2043 2017 2156 1037 3185 2066 2023 2005 1037 2235 5166 2017 2031 2000 2202 2008 1999 2000 4070 2043 2017 2024 10523 2009 1012 2045 2020 2070 2477 2008 2071 1997 2042 2488 2021 2087 2024 5166 3141 1012 1996 3772 2001 3492 2204 1996 1042 1013 1060 1998 28465 2020 2092 2589 1012 1037 3232 1997 3233 12166 2020 1996 3124 2040 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2245 1996 3185 2001 2941 3492 2204 1012 1045 5632 1996 3772 1998 2009 2333 2247 2092 1012 1996 2472 2790 2000 2428 10616 1996 2466 2002 2001 2667 2000 2425 1012 1045 2031 2000 2156 1996 2502 5166 2028 2746 2041 2651 1010 5525 2027 2018 1037 2843 2062 2769 2000 5466 2012 2009 2021 2001 2200 3422 3085 1012 2043 2017 2156 1037 3185 2066 2023 2005 1037 2235 5166 2017 2031 2000 2202 2008 1999 2000 4070 2043 2017 2024 10523 2009 1012 2045 2020 2070 2477 2008 2071 1997 2042 2488 2021 2087 2024 5166 3141 1012 1996 3772 2001 3492 2204 1996 1042 1013 1060 1998 28465 2020 2092 2589 1012 1037 3232 1997 3233 12166 2020 1996 3124 2040 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this contains spoil ##ers . < br / > < br / > i have rarely seen a film that is as unbelievable as this one is . and being french , it tries for depth by being enigma ##tic nothing really makes any sense . < br / > < br / > leo is gay and has just announced at breakfast to his family that he is hiv positive . the youngest brother , marcel , has not yet come down to breakfast , and the first thing the family does is decide that at 12 years of age he is too young to be told . maybe if he was four or five , but twelve ? the first totally unbelievable thing is [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this contains spoil ##ers . < br / > < br / > i have rarely seen a film that is as unbelievable as this one is . and being french , it tries for depth by being enigma ##tic nothing really makes any sense . < br / > < br / > leo is gay and has just announced at breakfast to his family that he is hiv positive . the youngest brother , marcel , has not yet come down to breakfast , and the first thing the family does is decide that at 12 years of age he is too young to be told . maybe if he was four or five , but twelve ? the first totally unbelievable thing is [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3397 27594 2545 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2031 6524 2464 1037 2143 2008 2003 2004 23653 2004 2023 2028 2003 1012 1998 2108 2413 1010 2009 5363 2005 5995 2011 2108 26757 4588 2498 2428 3084 2151 3168 1012 1026 7987 1013 1028 1026 7987 1013 1028 6688 2003 5637 1998 2038 2074 2623 2012 6350 2000 2010 2155 2008 2002 2003 9820 3893 1012 1996 6587 2567 1010 13389 1010 2038 2025 2664 2272 2091 2000 6350 1010 1998 1996 2034 2518 1996 2155 2515 2003 5630 2008 2012 2260 2086 1997 2287 2002 2003 2205 2402 2000 2022 2409 1012 2672 2065 2002 2001 2176 2030 2274 1010 2021 4376 1029 1996 2034 6135 23653 2518 2003 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3397 27594 2545 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2031 6524 2464 1037 2143 2008 2003 2004 23653 2004 2023 2028 2003 1012 1998 2108 2413 1010 2009 5363 2005 5995 2011 2108 26757 4588 2498 2428 3084 2151 3168 1012 1026 7987 1013 1028 1026 7987 1013 1028 6688 2003 5637 1998 2038 2074 2623 2012 6350 2000 2010 2155 2008 2002 2003 9820 3893 1012 1996 6587 2567 1010 13389 1010 2038 2025 2664 2272 2091 2000 6350 1010 1998 1996 2034 2518 1996 2155 2515 2003 5630 2008 2012 2260 2086 1997 2287 2002 2003 2205 2402 2000 2022 2409 1012 2672 2065 2002 2001 2176 2030 2274 1010 2021 4376 1029 1996 2034 6135 23653 2518 2003 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie ' s one red ##eem ##able quality ( besides at ##or ' s barely - there lo ##in ##cloth ) is the hilarious acting on the part of the bad guy , z ##or . this wonderful ##ly over ##play ##ed villain has a certain . . . oh , shakespeare ##an presence that made this movie bear ##able ( hence the 2 ) . i just giggled every time he pi ##rou ##ette ##d , lifting an inc ##red ##ulous eyebrow to hen ##chman or hero . a true example of someone not getting paid enough . ( and that beard ! ) < br / > < br / > now really , what was with the 12 - minute hang - [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie ' s one red ##eem ##able quality ( besides at ##or ' s barely - there lo ##in ##cloth ) is the hilarious acting on the part of the bad guy , z ##or . this wonderful ##ly over ##play ##ed villain has a certain . . . oh , shakespeare ##an presence that made this movie bear ##able ( hence the 2 ) . i just giggled every time he pi ##rou ##ette ##d , lifting an inc ##red ##ulous eyebrow to hen ##chman or hero . a true example of someone not getting paid enough . ( and that beard ! ) < br / > < br / > now really , what was with the 12 - minute hang - [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 1005 1055 2028 2417 21564 3085 3737 1006 4661 2012 2953 1005 1055 4510 1011 2045 8840 2378 23095 1007 2003 1996 26316 3772 2006 1996 2112 1997 1996 2919 3124 1010 1062 2953 1012 2023 6919 2135 2058 13068 2098 12700 2038 1037 3056 1012 1012 1012 2821 1010 8101 2319 3739 2008 2081 2023 3185 4562 3085 1006 6516 1996 1016 1007 1012 1045 2074 15889 2296 2051 2002 14255 22494 7585 2094 1010 8783 2019 4297 5596 16203 9522 2000 21863 19944 2030 5394 1012 1037 2995 2742 1997 2619 2025 2893 3825 2438 1012 1006 1998 2008 10154 999 1007 1026 7987 1013 1028 1026 7987 1013 1028 2085 2428 1010 2054 2001 2007 1996 2260 1011 3371 6865 1011 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 1005 1055 2028 2417 21564 3085 3737 1006 4661 2012 2953 1005 1055 4510 1011 2045 8840 2378 23095 1007 2003 1996 26316 3772 2006 1996 2112 1997 1996 2919 3124 1010 1062 2953 1012 2023 6919 2135 2058 13068 2098 12700 2038 1037 3056 1012 1012 1012 2821 1010 8101 2319 3739 2008 2081 2023 3185 4562 3085 1006 6516 1996 1016 1007 1012 1045 2074 15889 2296 2051 2002 14255 22494 7585 2094 1010 8783 2019 4297 5596 16203 9522 2000 21863 19944 2030 5394 1012 1037 2995 2742 1997 2619 2025 2893 3825 2438 1012 1006 1998 2008 10154 999 1007 1026 7987 1013 1028 1026 7987 1013 1028 2085 2428 1010 2054 2001 2007 1996 2260 1011 3371 6865 1011 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ka ##hin p ##ya ##ar na ho ja ##ay ##e is a great family movie . salman khan is looking handsome and great than ever ! there ' s even a scene where he takes off his shirt ! what a surprise ! ! ! rani mukherjee is great too . po ##oja bat ##ra had very few lines to say but i ' m glad she has been acknowledged for her role because she definitely has potential . < br / > < br / > it ' s about prem ( salman khan ) and he is a wedding singer . he is about to marry ni ##sha ( rave ##ena tan ##don ) but gets stood up . prem goes to ni ##sha [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ka ##hin p ##ya ##ar na ho ja ##ay ##e is a great family movie . salman khan is looking handsome and great than ever ! there ' s even a scene where he takes off his shirt ! what a surprise ! ! ! rani mukherjee is great too . po ##oja bat ##ra had very few lines to say but i ' m glad she has been acknowledged for her role because she definitely has potential . < br / > < br / > it ' s about prem ( salman khan ) and he is a wedding singer . he is about to marry ni ##sha ( rave ##ena tan ##don ) but gets stood up . prem goes to ni ##sha [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10556 10606 1052 3148 2906 6583 7570 14855 4710 2063 2003 1037 2307 2155 3185 1012 28542 4967 2003 2559 8502 1998 2307 2084 2412 999 2045 1005 1055 2130 1037 3496 2073 2002 3138 2125 2010 3797 999 2054 1037 4474 999 999 999 21617 27040 2003 2307 2205 1012 13433 22918 7151 2527 2018 2200 2261 3210 2000 2360 2021 1045 1005 1049 5580 2016 2038 2042 8969 2005 2014 2535 2138 2016 5791 2038 4022 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 2055 26563 1006 28542 4967 1007 1998 2002 2003 1037 5030 3220 1012 2002 2003 2055 2000 5914 9152 7377 1006 23289 8189 9092 5280 1007 2021 4152 2768 2039 1012 26563 3632 2000 9152 7377 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10556 10606 1052 3148 2906 6583 7570 14855 4710 2063 2003 1037 2307 2155 3185 1012 28542 4967 2003 2559 8502 1998 2307 2084 2412 999 2045 1005 1055 2130 1037 3496 2073 2002 3138 2125 2010 3797 999 2054 1037 4474 999 999 999 21617 27040 2003 2307 2205 1012 13433 22918 7151 2527 2018 2200 2261 3210 2000 2360 2021 1045 1005 1049 5580 2016 2038 2042 8969 2005 2014 2535 2138 2016 5791 2038 4022 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 2055 26563 1006 28542 4967 1007 1998 2002 2003 1037 5030 3220 1012 2002 2003 2055 2000 5914 9152 7377 1006 23289 8189 9092 5280 1007 2021 4152 2768 2039 1012 26563 3632 2000 9152 7377 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is brilliant . . . . . . without a doubt . watched it a while ago after constant pest ##ering from family members who are right into their sci - fi films ( which i am not ) , and thought it was quite good . but after recently watching a few documentaries on outer - space etc we watched it again . . . and it is good . < br / > < br / > kevin space ##y is without doubt one of the greatest actors ever and i really like jeff bridges ( big lew ##bow ##ski , blown away , arlington road ) . the film revolves around a patient in a nursing home who claims he is [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is brilliant . . . . . . without a doubt . watched it a while ago after constant pest ##ering from family members who are right into their sci - fi films ( which i am not ) , and thought it was quite good . but after recently watching a few documentaries on outer - space etc we watched it again . . . and it is good . < br / > < br / > kevin space ##y is without doubt one of the greatest actors ever and i really like jeff bridges ( big lew ##bow ##ski , blown away , arlington road ) . the film revolves around a patient in a nursing home who claims he is [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 8235 1012 1012 1012 1012 1012 1012 2302 1037 4797 1012 3427 2009 1037 2096 3283 2044 5377 20739 7999 2013 2155 2372 2040 2024 2157 2046 2037 16596 1011 10882 3152 1006 2029 1045 2572 2025 1007 1010 1998 2245 2009 2001 3243 2204 1012 2021 2044 3728 3666 1037 2261 15693 2006 6058 1011 2686 4385 2057 3427 2009 2153 1012 1012 1012 1998 2009 2003 2204 1012 1026 7987 1013 1028 1026 7987 1013 1028 4901 2686 2100 2003 2302 4797 2028 1997 1996 4602 5889 2412 1998 1045 2428 2066 5076 7346 1006 2502 24992 18912 5488 1010 10676 2185 1010 13929 2346 1007 1012 1996 2143 19223 2105 1037 5776 1999 1037 8329 2188 2040 4447 2002 2003 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 8235 1012 1012 1012 1012 1012 1012 2302 1037 4797 1012 3427 2009 1037 2096 3283 2044 5377 20739 7999 2013 2155 2372 2040 2024 2157 2046 2037 16596 1011 10882 3152 1006 2029 1045 2572 2025 1007 1010 1998 2245 2009 2001 3243 2204 1012 2021 2044 3728 3666 1037 2261 15693 2006 6058 1011 2686 4385 2057 3427 2009 2153 1012 1012 1012 1998 2009 2003 2204 1012 1026 7987 1013 1028 1026 7987 1013 1028 4901 2686 2100 2003 2302 4797 2028 1997 1996 4602 5889 2412 1998 1045 2428 2066 5076 7346 1006 2502 24992 18912 5488 1010 10676 2185 1010 13929 2346 1007 1012 1996 2143 19223 2105 1037 5776 1999 1037 8329 2188 2040 4447 2002 2003 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] oh , well i thought it should be a good action , but it was not . although jeff speak ##man stars there is nothing to watch . only two fight for almost 1 , 5 hours , ya ##k . a lot of talking and everything is so artificial that you could not believe it . the plot is clear from the beginning . if you want good action don ' t rent this movie . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] oh , well i thought it should be a good action , but it was not . although jeff speak ##man stars there is nothing to watch . only two fight for almost 1 , 5 hours , ya ##k . a lot of talking and everything is so artificial that you could not believe it . the plot is clear from the beginning . if you want good action don ' t rent this movie . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2821 1010 2092 1045 2245 2009 2323 2022 1037 2204 2895 1010 2021 2009 2001 2025 1012 2348 5076 3713 2386 3340 2045 2003 2498 2000 3422 1012 2069 2048 2954 2005 2471 1015 1010 1019 2847 1010 8038 2243 1012 1037 2843 1997 3331 1998 2673 2003 2061 7976 2008 2017 2071 2025 2903 2009 1012 1996 5436 2003 3154 2013 1996 2927 1012 2065 2017 2215 2204 2895 2123 1005 1056 9278 2023 3185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2821 1010 2092 1045 2245 2009 2323 2022 1037 2204 2895 1010 2021 2009 2001 2025 1012 2348 5076 3713 2386 3340 2045 2003 2498 2000 3422 1012 2069 2048 2954 2005 2471 1015 1010 1019 2847 1010 8038 2243 1012 1037 2843 1997 3331 1998 2673 2003 2061 7976 2008 2017 2071 2025 2903 2009 1012 1996 5436 2003 3154 2013 1996 2927 1012 2065 2017 2215 2204 2895 2123 1005 1056 9278 2023 3185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is really a big piece of trash trying to make itself look like a hollywood production . poor story outline ( stupid robot story ) . . . ultra bad acting by un ##tale ##nted pop idols . . . and they are trying to \" fight \" ! ! ! my goodness . . . those miserable actors uses wires to make them look like they are \" good fighters \" . . . : ( and i hate that arrogant edison chen . . . the worst actor i have ever seen ! ! ! i will never touch his movies again . avoid this movie at all costs ! ! ! i wanted to give it a negative value out of [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is really a big piece of trash trying to make itself look like a hollywood production . poor story outline ( stupid robot story ) . . . ultra bad acting by un ##tale ##nted pop idols . . . and they are trying to \" fight \" ! ! ! my goodness . . . those miserable actors uses wires to make them look like they are \" good fighters \" . . . : ( and i hate that arrogant edison chen . . . the worst actor i have ever seen ! ! ! i will never touch his movies again . avoid this movie at all costs ! ! ! i wanted to give it a negative value out of [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 2428 1037 2502 3538 1997 11669 2667 2000 2191 2993 2298 2066 1037 5365 2537 1012 3532 2466 12685 1006 5236 8957 2466 1007 1012 1012 1012 11087 2919 3772 2011 4895 22059 14706 3769 24438 1012 1012 1012 1998 2027 2024 2667 2000 1000 2954 1000 999 999 999 2026 15003 1012 1012 1012 2216 13736 5889 3594 14666 2000 2191 2068 2298 2066 2027 2024 1000 2204 7299 1000 1012 1012 1012 1024 1006 1998 1045 5223 2008 15818 17046 8802 1012 1012 1012 1996 5409 3364 1045 2031 2412 2464 999 999 999 1045 2097 2196 3543 2010 5691 2153 1012 4468 2023 3185 2012 2035 5366 999 999 999 1045 2359 2000 2507 2009 1037 4997 3643 2041 1997 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 2428 1037 2502 3538 1997 11669 2667 2000 2191 2993 2298 2066 1037 5365 2537 1012 3532 2466 12685 1006 5236 8957 2466 1007 1012 1012 1012 11087 2919 3772 2011 4895 22059 14706 3769 24438 1012 1012 1012 1998 2027 2024 2667 2000 1000 2954 1000 999 999 999 2026 15003 1012 1012 1012 2216 13736 5889 3594 14666 2000 2191 2068 2298 2066 2027 2024 1000 2204 7299 1000 1012 1012 1012 1024 1006 1998 1045 5223 2008 15818 17046 8802 1012 1012 1012 1996 5409 3364 1045 2031 2412 2464 999 999 999 1045 2097 2196 3543 2010 5691 2153 1012 4468 2023 3185 2012 2035 5366 999 999 999 1045 2359 2000 2507 2009 1037 4997 3643 2041 1997 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3886dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
